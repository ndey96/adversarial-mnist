{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "X_train = mnist.train.images  # Returns np.array\n",
    "y_train = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "X_test = mnist.test.images  # Returns np.array\n",
    "y_test = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "to_categorical = tf.keras.utils.to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing graph\n",
      "Training\n",
      "Train model\n",
      "\n",
      "Epoch 1/1\n",
      "Saving model\n",
      "Evaluating on clean data\n",
      "Evaluating\n",
      " loss: 2.0750 acc: 0.2530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.075032, 0.253)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(x, logits=False, training=False):\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "    input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=training)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits_ = tf.layers.dense(inputs=dropout, units=10)\n",
    "    y = tf.nn.softmax(logits_, name=\"softmax_tensor\")\n",
    "    \n",
    "    if logits:\n",
    "        return y, logits_\n",
    "    return y\n",
    "\n",
    "class Dummy:\n",
    "    pass\n",
    "\n",
    "env = Dummy()\n",
    "env.x = tf.placeholder(tf.float32, (None, 784), name='x')\n",
    "env.y = tf.placeholder(tf.float32, (None, 10), name='y')\n",
    "env.training = tf.placeholder_with_default(False, (), name='mode')\n",
    "env.ybar, logits = model(env.x, logits=True, training=env.training)\n",
    "count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n",
    "env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name='acc')\n",
    "xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y, logits=logits)\n",
    "env.loss = tf.reduce_mean(xent, name='loss')\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "env.train_op = optimizer.minimize(env.loss)\n",
    "env.saver = tf.train.Saver()\n",
    "\n",
    "print('Initializing graph')\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "                                               \n",
    "def evaluate(sess, env, X_data, y_data, batch_size=128):\n",
    "    print('Evaluating')\n",
    "    loss, acc = sess.run([env.loss, env.acc], feed_dict={env.x: X_data, env.y: y_data})\n",
    "    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n",
    "    return loss, acc\n",
    "\n",
    "def train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1, load=False, shuffle=False, batch_size=128, name='model'):\n",
    "\n",
    "    if load:\n",
    "        if not hasattr(env, 'saver'):\n",
    "            print('Error: cannot find saver op')\n",
    "            return\n",
    "        print('Loading saved model')\n",
    "        return env.saver.restore(sess, 'model/{}'.format(name))\n",
    "\n",
    "    print('Train model')\n",
    "    for epoch in range(epochs):\n",
    "        print('\\nEpoch {0}/{1}'.format(epoch + 1, epochs))\n",
    "        sess.run(env.train_op, feed_dict={env.x: X_data,\n",
    "                                              env.y: y_data,\n",
    "                                              env.training: True})\n",
    "\n",
    "    if hasattr(env, 'saver'):\n",
    "        print('Saving model')\n",
    "#         os.mkdir('model')\n",
    "        env.saver.save(sess, 'model/{}'.format(name))\n",
    "                                               \n",
    "def predict(sess, env, X_data):\n",
    "    print('Predicting')\n",
    "    yval = sess.run(env.ybar, feed_dict={env.x: X_data})\n",
    "    return yval\n",
    "\n",
    "print('Training')\n",
    "\n",
    "train(sess, env, X_train, y_train, load=False, epochs=1, name='mnist')\n",
    "\n",
    "print('Evaluating on clean data')\n",
    "\n",
    "evaluate(sess, env, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.09406996, 0.06860119, 0.09676023, 0.13229975, 0.0853923 ,\n",
       "        0.07464116, 0.081567  , 0.14259279, 0.11682194, 0.10725367],\n",
       "       [0.10500295, 0.06399569, 0.13599719, 0.17741226, 0.0686805 ,\n",
       "        0.07716009, 0.10072666, 0.07264616, 0.12114762, 0.07723088]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(sess, env, X_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
